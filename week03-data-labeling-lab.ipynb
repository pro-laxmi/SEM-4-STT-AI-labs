{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Lab: Data Labeling & Annotation\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**  \n",
    "**IIT Gandhinagar**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Set up and use Label Studio for annotation tasks\n",
    "2. Create annotation interfaces for different data types\n",
    "3. Write clear annotation guidelines\n",
    "4. Calculate Inter-Annotator Agreement (IAA) metrics\n",
    "5. Apply Cohen's Kappa and Fleiss' Kappa to measure label quality\n",
    "6. Calculate IoU for spatial annotations\n",
    "\n",
    "---\n",
    "\n",
    "## Netflix Movie Theme\n",
    "\n",
    "Continuing from Weeks 1-2, we'll label our cleaned movie reviews for sentiment analysis. This labeled data will be used for model training in later weeks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "### 1.1 Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install label-studio scikit-learn statsmodels pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Sample Data for Annotation\n",
    "\n",
    "### 2.1 Create Movie Review Dataset\n",
    "\n",
    "We'll create a set of movie reviews to annotate for sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample movie reviews for annotation\n",
    "movie_reviews = [\n",
    "    {\"id\": 1, \"movie\": \"Inception\", \"Casts\": \"Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page\", \"review\": \"Mind-blowing! Nolan does it again with this masterpiece.\"},\n",
    "    {\"id\": 2, \"movie\": \"The Room\", \"Casts\": \"Tommy Wiseau, Juliette Danielle, Greg Sestero\", \"review\": \"So bad it's good. Hilarious unintentionally.\"},\n",
    "    {\"id\": 3, \"movie\": \"Parasite\", \"Casts\": \"Song Kang-ho, Lee Sun-kyun, Choi Woo-shik\", \"review\": \"Gripping from start to finish. Deserved every Oscar.\"},\n",
    "    {\"id\": 4, \"movie\": \"Cats\", \"Casts\": \"James Corden, Judi Dench, Ian McKellen\", \"review\": \"What did I just watch? Truly bizarre.\"},\n",
    "    {\"id\": 5, \"movie\": \"The Godfather\", \"Casts\": \"Marlon Brando, Al Paccino, James Caan\", \"review\": \"A timeless classic. Perfect in every way.\"},\n",
    "    {\"id\": 6, \"movie\": \"Avatar\", \"Casts\": \"Sam Worthington, Zoe Saldana, Sigourney Weaver\", \"review\": \"Visually stunning but the story is predictable.\"},\n",
    "    {\"id\": 7, \"movie\": \"The Dark Knight\", \"Casts\": \"Christian Bale, Heath Ledger, Aaron Eckhart\", \"review\": \"Heath Ledger's Joker is unforgettable.\"},\n",
    "    {\"id\": 8, \"movie\": \"Twilight\",  \"Casts\": \"Kristen Stewart, Robert Pattinson, Taylor Lautner\", \"review\": \"Not my cup of tea but I can see the appeal.\"},\n",
    "    {\"id\": 9, \"movie\": \"Interstellar\", \"Casts\": \"Matthew McConaughey, Anne Hathaway, Jessica Chastain\", \"review\": \"Made me cry. Beautiful exploration of love and time.\"},\n",
    "    {\"id\": 10, \"movie\": \"Emoji Movie\", \"Casts\": \"T.J. Miller, James Corden, Anna Faris\", \"review\": \"Just... no. Avoid at all costs.\"},\n",
    "    {\"id\": 11, \"movie\": \"Pulp Fiction\", \"Casts\": \"John Travolta, Samuel L. Jackson, Uma Thurman\", \"review\": \"Tarantino's dialogue is unmatched.\"},\n",
    "    {\"id\": 12, \"movie\": \"Sharknado\",  \"Casts\": \"Ian Ziering, Tara Reid, John Heard\", \"review\": \"Ridiculous premise but entertaining in a weird way.\"},\n",
    "    {\"id\": 13, \"movie\": \"The Shawshank Redemption\", \"Casts\": \"Tim Robbins, Morgan Freeman, Bob Gunton\", \"review\": \"Hope is a good thing. Best movie ever made.\"},\n",
    "    {\"id\": 14, \"movie\": \"Transformers 5\", \"Casts\": \"Mark Wahlberg, Anthony Hopkins, Isabela Moner\", \"review\": \"Explosions. That's it. That's the review.\"},\n",
    "    {\"id\": 15, \"movie\": \"La La Land\", \"Casts\": \"Ryan Gosling, Emma Stone, John Legend\", \"review\": \"Bittersweet ending that stays with you.\"},\n",
    "    {\"id\": 16, \"movie\": \"Batman v Superman\",  \"Casts\": \"Ben Affleck, Henry Cavill, Amy Adams\", \"review\": \"Had potential but ended up being a mess.\"},\n",
    "    {\"id\": 17, \"movie\": \"Spirited Away\", \"Casts\": \"Rumi Hiiragi, Miyu Irino, Mari Natsuki\", \"review\": \"Miyazaki's imagination knows no bounds.\"},\n",
    "    {\"id\": 18, \"movie\": \"The Last Airbender\", \"Casts\": \"Noah Ringer, Nicola Peltz, Jackson Rathbone\", \"review\": \"An insult to the animated series fans.\"},\n",
    "    {\"id\": 19, \"movie\": \"Coco\", \"Casts\": \"Anthony Gonzalez, Gael García Bernal, Benjamin Bratt\", \"review\": \"Pixar at its finest. Remember me...\"},\n",
    "    {\"id\": 20, \"movie\": \"Justice League\", \"Casts\": \"Ben Affleck, Henry Cavill, Gal Gadot\", \"review\": \"Okay but nothing special. Just okay.\"},\n",
    "]\n",
    "\n",
    "df_reviews = pd.DataFrame(movie_reviews)\n",
    "print(f\"Created {len(df_reviews)} movie reviews for annotation\")\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 (Solved): Export Data for Label Studio\n",
    "\n",
    "Export the reviews in Label Studio's expected JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Export for Label Studio\n",
    "def export_for_label_studio(df, text_column='review'):\n",
    "    \"\"\"\n",
    "    Export DataFrame to Label Studio JSON format.\n",
    "    \n",
    "    Label Studio expects a list of objects with a 'data' field.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for _, row in df.iterrows():\n",
    "        task = {\n",
    "            \"data\": {\n",
    "                \"id\": int(row['id']),\n",
    "                \"movie\": row['movie'],\n",
    "                \"text\": row[text_column]\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    return tasks\n",
    "\n",
    "tasks = export_for_label_studio(df_reviews)\n",
    "\n",
    "# Save to file\n",
    "with open('movie_reviews_for_labeling.json', 'w') as f:\n",
    "    json.dump(tasks, f, indent=2)\n",
    "\n",
    "print(\"Exported to movie_reviews_for_labeling.json\")\n",
    "print(\"\\nSample task:\")\n",
    "print(json.dumps(tasks[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Add More Fields for Export\n",
    "\n",
    "Modify the export function to also include the movie title in the Label Studio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify the export function to include movie title\n",
    "# The Label Studio interface should show both movie title and review\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Label Studio Configuration\n",
    "\n",
    "### 3.1 Understanding Label Studio XML Configs\n",
    "\n",
    "Label Studio uses XML to define annotation interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification Config for Sentiment Analysis\n",
    "sentiment_config = '''\n",
    "<View>\n",
    "  <Header value=\"Movie Review Sentiment Analysis\"/>\n",
    "  <Text name=\"text\" value=\"$text\"/>\n",
    "  <Choices name=\"sentiment\" toName=\"text\" choice=\"single\">\n",
    "    <Choice value=\"Positive\" hotkey=\"1\"/>\n",
    "    <Choice value=\"Negative\" hotkey=\"2\"/>\n",
    "    <Choice value=\"Neutral\" hotkey=\"3\"/>\n",
    "    <Choice value=\"Mixed\" hotkey=\"4\"/>\n",
    "  </Choices>\n",
    "</View>\n",
    "'''\n",
    "\n",
    "print(\"Sentiment Classification Config:\")\n",
    "print(sentiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (Solved): Create NER Config\n",
    "\n",
    "Create a Label Studio config for Named Entity Recognition on movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: NER Config for Movie Reviews\n",
    "ner_config = '''\n",
    "<View>\n",
    "  <Header value=\"Movie Entity Annotation\"/>\n",
    "  <Labels name=\"ner\" toName=\"text\">\n",
    "    <Label value=\"MOVIE_TITLE\" background=\"#FF0000\"/>\n",
    "    <Label value=\"ACTOR\" background=\"#00FF00\"/>\n",
    "    <Label value=\"DIRECTOR\" background=\"#0000FF\"/>\n",
    "    <Label value=\"CHARACTER\" background=\"#FFA500\"/>\n",
    "    <Label value=\"AWARD\" background=\"#800080\"/>\n",
    "  </Labels>\n",
    "  <Text name=\"text\" value=\"$text\"/>\n",
    "</View>\n",
    "'''\n",
    "\n",
    "print(\"NER Config for Movie Reviews:\")\n",
    "print(ner_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Create Multi-Label Classification Config\n",
    "\n",
    "Create a config that allows multiple labels per review (e.g., \"Funny\", \"Emotional\", \"Action-packed\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create multi-label classification config\n",
    "# Hint: Use choice=\"multiple\" instead of choice=\"single\"\n",
    "\n",
    "# Your config here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Create Image Classification Config\n",
    "\n",
    "Create a config for classifying movie poster genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create image classification config for movie poster genre classification\n",
    "# Include genres: Action, Comedy, Drama, Horror, Sci-Fi, Romance\n",
    "\n",
    "# Your config here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Annotation Guidelines\n",
    "\n",
    "### 4.1 Writing Clear Guidelines\n",
    "\n",
    "Good annotation guidelines are crucial for consistent labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example annotation guidelines\n",
    "annotation_guidelines = \"\"\"\n",
    "# Movie Review Sentiment Annotation Guidelines\n",
    "\n",
    "## Task Overview\n",
    "Classify movie reviews into sentiment categories based on the reviewer's opinion of the movie.\n",
    "\n",
    "## Label Definitions\n",
    "\n",
    "### POSITIVE\n",
    "The reviewer clearly enjoyed the movie and recommends it.\n",
    "\n",
    "**Examples:**\n",
    "- \"Mind-blowing! Nolan does it again with this masterpiece.\"\n",
    "- \"Best movie I've seen this year. A must-watch!\"\n",
    "- \"Perfect in every way. 10/10.\"\n",
    "\n",
    "### NEGATIVE\n",
    "The reviewer clearly did not enjoy the movie and does not recommend it.\n",
    "\n",
    "**Examples:**\n",
    "- \"Avoid at all costs. Complete waste of time.\"\n",
    "- \"An insult to the animated series fans.\"\n",
    "- \"Just... no. Terrible.\"\n",
    "\n",
    "### NEUTRAL\n",
    "The reviewer has no strong opinion either way, or the review is purely factual.\n",
    "\n",
    "**Examples:**\n",
    "- \"It's okay. Nothing special.\"\n",
    "- \"The movie is 2 hours long and features action scenes.\"\n",
    "- \"Just okay.\"\n",
    "\n",
    "### MIXED\n",
    "The review contains both positive and negative aspects.\n",
    "\n",
    "**Examples:**\n",
    "- \"Visually stunning but the story is predictable.\"\n",
    "- \"Great acting but poor direction.\"\n",
    "- \"Had potential but ended up being a mess.\"\n",
    "\n",
    "## Edge Cases\n",
    "\n",
    "### Sarcasm\n",
    "If sarcasm is detected, label based on the ACTUAL meaning, not the literal words.\n",
    "- \"Oh great, another superhero movie. Just what we needed.\" → NEGATIVE\n",
    "\n",
    "### \"So bad it's good\"\n",
    "Movies praised ironically should be labeled MIXED (enjoyment exists but film quality is poor).\n",
    "- \"So bad it's good. Hilarious unintentionally.\" → MIXED\n",
    "\n",
    "### Qualified Praise/Criticism\n",
    "If the reviewer has reservations, use MIXED.\n",
    "- \"Not my cup of tea but I can see the appeal.\" → NEUTRAL (not recommending but not criticizing)\n",
    "\n",
    "## When in Doubt\n",
    "1. Re-read the review\n",
    "2. Ask: \"Would the reviewer recommend this movie?\"\n",
    "3. If still unclear, use NEUTRAL\n",
    "\"\"\"\n",
    "\n",
    "print(annotation_guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Add Edge Cases to Guidelines\n",
    "\n",
    "What other edge cases should be added to these guidelines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write 3 additional edge cases that should be covered in the guidelines\n",
    "# Consider: emojis, ratings, comparisons to other movies, etc.\n",
    "\n",
    "additional_edge_cases = \"\"\"\n",
    "### Edge Case 1: [Your title]\n",
    "[Description and example]\n",
    "\n",
    "### Edge Case 2: [Your title]\n",
    "[Description and example]\n",
    "\n",
    "### Edge Case 3: [Your title]\n",
    "[Description and example]\n",
    "\"\"\"\n",
    "\n",
    "# Your edge cases here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Simulating Annotations\n",
    "\n",
    "### 5.1 Creating Simulated Annotator Data\n",
    "\n",
    "For this lab, we'll simulate multiple annotators to practice IAA calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated annotations from two annotators\n",
    "np.random.seed(42)\n",
    "\n",
    "# Ground truth labels (for simulation)\n",
    "ground_truth = ['Positive', 'Mixed', 'Positive', 'Negative', 'Positive',\n",
    "                'Mixed', 'Positive', 'Neutral', 'Positive', 'Negative',\n",
    "                'Positive', 'Mixed', 'Positive', 'Negative', 'Mixed',\n",
    "                'Negative', 'Positive', 'Negative', 'Positive', 'Neutral']\n",
    "\n",
    "# Annotator 1: 90% accuracy\n",
    "def simulate_annotator(true_labels, accuracy=0.9):\n",
    "    labels = ['Positive', 'Negative', 'Neutral', 'Mixed']\n",
    "    annotations = []\n",
    "    for true_label in true_labels:\n",
    "        if np.random.random() < accuracy:\n",
    "            annotations.append(true_label)\n",
    "        else:\n",
    "            # Random different label\n",
    "            other_labels = [l for l in labels if l != true_label]\n",
    "            annotations.append(np.random.choice(other_labels))\n",
    "    return annotations\n",
    "\n",
    "annotator1 = simulate_annotator(ground_truth, accuracy=0.85)\n",
    "annotator2 = simulate_annotator(ground_truth, accuracy=0.80)\n",
    "annotator3 = simulate_annotator(ground_truth, accuracy=0.75)\n",
    "\n",
    "# Create DataFrame\n",
    "annotations_df = pd.DataFrame({\n",
    "    'review_id': range(1, 21),\n",
    "    'annotator_1': annotator1,\n",
    "    'annotator_2': annotator2,\n",
    "    'annotator_3': annotator3,\n",
    "    'ground_truth': ground_truth\n",
    "})\n",
    "\n",
    "print(\"Simulated Annotations:\")\n",
    "annotations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 (Solved): Calculate Percent Agreement\n",
    "\n",
    "Calculate the simple percent agreement between annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Calculate percent agreement\n",
    "def percent_agreement(labels1, labels2):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of items where two annotators agree.\n",
    "    \"\"\"\n",
    "    agreements = sum(l1 == l2 for l1, l2 in zip(labels1, labels2))\n",
    "    return agreements / len(labels1)\n",
    "\n",
    "# Calculate pairwise agreements\n",
    "pa_1_2 = percent_agreement(annotator1, annotator2)\n",
    "pa_1_3 = percent_agreement(annotator1, annotator3)\n",
    "pa_2_3 = percent_agreement(annotator2, annotator3)\n",
    "\n",
    "print(f\"Percent Agreement (Annotator 1 vs 2): {pa_1_2:.2%}\")\n",
    "print(f\"Percent Agreement (Annotator 1 vs 3): {pa_1_3:.2%}\")\n",
    "print(f\"Percent Agreement (Annotator 2 vs 3): {pa_2_3:.2%}\")\n",
    "print(f\"\\nAverage Pairwise Agreement: {(pa_1_2 + pa_1_3 + pa_2_3) / 3:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2: Why is Percent Agreement Not Enough?\n",
    "\n",
    "Explain the limitation of percent agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your explanation here\n",
    "# Hint: Consider what happens with random guessing in a binary classification task\n",
    "\n",
    "explanation = \"\"\"\n",
    "Your explanation here...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Cohen's Kappa\n",
    "\n",
    "### 6.1 Understanding Cohen's Kappa\n",
    "\n",
    "Cohen's Kappa accounts for agreement by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohen's Kappa formula explanation\n",
    "print(\"\"\"\n",
    "Cohen's Kappa Formula:\n",
    "\n",
    "           P_observed - P_expected\n",
    "kappa = ────────────────────────────\n",
    "            1 - P_expected\n",
    "\n",
    "Where:\n",
    "- P_observed = Actual agreement rate\n",
    "- P_expected = Expected agreement by chance\n",
    "\n",
    "Interpretation:\n",
    "- kappa < 0.00: Poor (worse than chance)\n",
    "- 0.00 - 0.20: Slight\n",
    "- 0.21 - 0.40: Fair\n",
    "- 0.41 - 0.60: Moderate\n",
    "- 0.61 - 0.80: Substantial\n",
    "- 0.81 - 1.00: Almost Perfect\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1 (Solved): Calculate Cohen's Kappa Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Manual Cohen's Kappa calculation\n",
    "def cohens_kappa_manual(labels1, labels2):\n",
    "    \"\"\"\n",
    "    Calculate Cohen's Kappa step by step.\n",
    "    \"\"\"\n",
    "    # Step 1: Create confusion matrix\n",
    "    unique_labels = sorted(set(labels1) | set(labels2))\n",
    "    n = len(labels1)\n",
    "    \n",
    "    # Count agreements and marginals\n",
    "    confusion = {}\n",
    "    for l1 in unique_labels:\n",
    "        confusion[l1] = {l2: 0 for l2 in unique_labels}\n",
    "    \n",
    "    for l1, l2 in zip(labels1, labels2):\n",
    "        confusion[l1][l2] += 1\n",
    "    \n",
    "    # Step 2: Calculate observed agreement\n",
    "    p_observed = sum(confusion[l][l] for l in unique_labels) / n\n",
    "    \n",
    "    # Step 3: Calculate expected agreement\n",
    "    marginals_1 = {l: sum(1 for x in labels1 if x == l) / n for l in unique_labels}\n",
    "    marginals_2 = {l: sum(1 for x in labels2 if x == l) / n for l in unique_labels}\n",
    "    \n",
    "    p_expected = sum(marginals_1[l] * marginals_2[l] for l in unique_labels)\n",
    "    \n",
    "    # Step 4: Calculate kappa\n",
    "    kappa = (p_observed - p_expected) / (1 - p_expected)\n",
    "    \n",
    "    print(f\"Observed Agreement (P_o): {p_observed:.4f}\")\n",
    "    print(f\"Expected Agreement (P_e): {p_expected:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "    \n",
    "    return kappa\n",
    "\n",
    "print(\"Annotator 1 vs Annotator 2:\")\n",
    "kappa_1_2 = cohens_kappa_manual(annotator1, annotator2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Verify with sklearn\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "sklearn_kappa = cohen_kappa_score(annotator1, annotator2)\n",
    "print(f\"Sklearn Cohen's Kappa: {sklearn_kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.2: Calculate All Pairwise Kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate Cohen's Kappa for all pairs of annotators\n",
    "# Use sklearn's cohen_kappa_score function\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.3: Visualize Agreement with Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a confusion matrix heatmap for Annotator 1 vs Annotator 2\n",
    "# Hint: Use sklearn's confusion_matrix and seaborn's heatmap\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Fleiss' Kappa for Multiple Annotators\n",
    "\n",
    "### 7.1 Understanding Fleiss' Kappa\n",
    "\n",
    "When you have more than 2 annotators, use Fleiss' Kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fleiss' Kappa using statsmodels\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "\n",
    "# Prepare data for Fleiss' Kappa\n",
    "# Format: (n_items, n_categories) matrix with counts\n",
    "\n",
    "labels = ['Positive', 'Negative', 'Neutral', 'Mixed']\n",
    "n_items = len(annotator1)\n",
    "\n",
    "# Create matrix\n",
    "data_matrix = []\n",
    "for i in range(n_items):\n",
    "    row = [0, 0, 0, 0]  # Counts for each category\n",
    "    for ann in [annotator1[i], annotator2[i], annotator3[i]]:\n",
    "        row[labels.index(ann)] += 1\n",
    "    data_matrix.append(row)\n",
    "\n",
    "data_matrix = np.array(data_matrix)\n",
    "print(\"Data matrix shape:\", data_matrix.shape)\n",
    "print(\"\\nFirst 5 rows (counts per category):\")\n",
    "print(pd.DataFrame(data_matrix[:5], columns=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7.1 (Solved): Calculate Fleiss' Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Calculate Fleiss' Kappa\n",
    "fk = fleiss_kappa(data_matrix)\n",
    "print(f\"Fleiss' Kappa (3 annotators): {fk:.4f}\")\n",
    "\n",
    "# Interpret\n",
    "if fk < 0:\n",
    "    print(\"Interpretation: Poor (worse than chance)\")\n",
    "elif fk < 0.20:\n",
    "    print(\"Interpretation: Slight agreement\")\n",
    "elif fk < 0.40:\n",
    "    print(\"Interpretation: Fair agreement\")\n",
    "elif fk < 0.60:\n",
    "    print(\"Interpretation: Moderate agreement\")\n",
    "elif fk < 0.80:\n",
    "    print(\"Interpretation: Substantial agreement\")\n",
    "else:\n",
    "    print(\"Interpretation: Almost perfect agreement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7.2: How Would You Improve Agreement?\n",
    "\n",
    "Based on the Fleiss' Kappa score, what actions would you take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write 3 specific actions to improve annotator agreement\n",
    "\n",
    "improvement_actions = \"\"\"\n",
    "1. [Your action]\n",
    "\n",
    "2. [Your action]\n",
    "\n",
    "3. [Your action]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: IoU for Spatial Annotations\n",
    "\n",
    "### 8.1 Intersection over Union (IoU)\n",
    "\n",
    "For bounding boxes and segmentation, we use IoU to measure agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU explanation\n",
    "print(\"\"\"\n",
    "Intersection over Union (IoU):\n",
    "\n",
    "           Area of Overlap\n",
    "IoU = ─────────────────────────\n",
    "         Area of Union\n",
    "\n",
    "      ┌─────────────┐\n",
    "      │    ┌────────┼───────┐\n",
    "      │    │////////│       │\n",
    "      │    │//Ovrlp/│       │\n",
    "      └────┼────────┘       │\n",
    "           │                │\n",
    "           └────────────────┘\n",
    "\n",
    "IoU = Overlap / (Box1 + Box2 - Overlap)\n",
    "\n",
    "Thresholds:\n",
    "- IoU > 0.5: Match (standard)\n",
    "- IoU > 0.7: Good match\n",
    "- IoU > 0.9: Excellent match\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.1 (Solved): Implement IoU Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: IoU Implementation\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        box1, box2: Lists of [x1, y1, x2, y2] coordinates\n",
    "    \n",
    "    Returns:\n",
    "        IoU value (0 to 1)\n",
    "    \"\"\"\n",
    "    # Calculate intersection coordinates\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # Check for no overlap\n",
    "    if x2 < x1 or y2 < y1:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate areas\n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "# Test cases\n",
    "box1 = [0, 0, 100, 100]  # 100x100 box at origin\n",
    "box2 = [50, 50, 150, 150]  # 100x100 box shifted by 50\n",
    "box3 = [200, 200, 300, 300]  # No overlap\n",
    "box4 = [25, 25, 75, 75]  # Contained within box1\n",
    "\n",
    "print(f\"IoU(box1, box2): {calculate_iou(box1, box2):.4f}\")  # Partial overlap\n",
    "print(f\"IoU(box1, box3): {calculate_iou(box1, box3):.4f}\")  # No overlap\n",
    "print(f\"IoU(box1, box4): {calculate_iou(box1, box4):.4f}\")  # One inside other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.2: Calculate IoU for Movie Poster Annotations\n",
    "\n",
    "Two annotators drew bounding boxes around movie titles on posters. Calculate their IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box annotations from two annotators\n",
    "poster_annotations = [\n",
    "    {\"poster\": \"Inception\", \"ann1\": [100, 50, 300, 100], \"ann2\": [95, 55, 305, 98]},\n",
    "    {\"poster\": \"The Matrix\", \"ann1\": [50, 100, 250, 180], \"ann2\": [60, 105, 240, 175]},\n",
    "    {\"poster\": \"Avatar\", \"ann1\": [120, 80, 280, 150], \"ann2\": [125, 78, 275, 155]},\n",
    "    {\"poster\": \"Titanic\", \"ann1\": [80, 60, 320, 120], \"ann2\": [100, 70, 310, 115]},\n",
    "]\n",
    "\n",
    "# TODO: Calculate IoU for each poster and compute the mean IoU\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.3: Visualize Bounding Box Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a visualization showing two overlapping bounding boxes\n",
    "# Use matplotlib to draw rectangles\n",
    "# Hint: matplotlib.patches.Rectangle\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Majority Voting and Adjudication\n",
    "\n",
    "### 9.1 Resolving Disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority voting implementation\n",
    "def majority_vote(annotations):\n",
    "    \"\"\"\n",
    "    Return the most common label from a list of annotations.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    counts = Counter(annotations)\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "# Apply majority voting\n",
    "final_labels = []\n",
    "for i in range(len(annotator1)):\n",
    "    annotations = [annotator1[i], annotator2[i], annotator3[i]]\n",
    "    final_labels.append(majority_vote(annotations))\n",
    "\n",
    "annotations_df['majority_vote'] = final_labels\n",
    "print(\"Labels with Majority Vote:\")\n",
    "annotations_df[['review_id', 'annotator_1', 'annotator_2', 'annotator_3', 'majority_vote', 'ground_truth']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9.1: Identify Disagreements for Expert Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify reviews where all 3 annotators disagree (no majority)\n",
    "# These should be sent to an expert for adjudication\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9.2: Implement Weighted Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement weighted voting where annotator votes are weighted by their accuracy\n",
    "# Annotator 1 accuracy: 85%, Annotator 2: 80%, Annotator 3: 75%\n",
    "\n",
    "def weighted_vote(annotations, weights):\n",
    "    \"\"\"\n",
    "    Return the label with highest weighted vote.\n",
    "    \n",
    "    Args:\n",
    "        annotations: List of labels from each annotator\n",
    "        weights: List of weights for each annotator\n",
    "    \n",
    "    Returns:\n",
    "        Label with highest weighted sum\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Quality Dashboard\n",
    "\n",
    "### Question 10.1: Create an Annotator Quality Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a quality dashboard showing:\n",
    "# 1. Each annotator's accuracy vs ground truth\n",
    "# 2. Pairwise kappa scores\n",
    "# 3. Number of labels per category for each annotator\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems\n",
    "\n",
    "### Challenge 1: Krippendorff's Alpha\n",
    "\n",
    "Implement Krippendorff's Alpha, which handles missing data and ordinal categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement Krippendorff's Alpha\n",
    "# This metric handles:\n",
    "# - Any number of annotators\n",
    "# - Missing data\n",
    "# - Different types of data (nominal, ordinal, interval, ratio)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Annotation Cost Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Build a function that estimates annotation cost\n",
    "# Input: number of items, task type, quality level (redundancy)\n",
    "# Output: estimated cost in USD and time in hours\n",
    "\n",
    "def estimate_annotation_cost(n_items, task_type, quality_level, domain='general'):\n",
    "    \"\"\"\n",
    "    Estimate the cost and time for an annotation project.\n",
    "    \n",
    "    Args:\n",
    "        n_items: Number of items to annotate\n",
    "        task_type: 'text_classification', 'ner', 'bbox', 'segmentation'\n",
    "        quality_level: 'low' (1 annotator), 'medium' (2), 'high' (3)\n",
    "        domain: 'general' or 'expert'\n",
    "    \n",
    "    Returns:\n",
    "        dict with cost_usd, time_hours, annotators_needed\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test with:\n",
    "# estimate_annotation_cost(10000, 'text_classification', 'high')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Label Studio API Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read label studio API documentation here: https://api.labelstud.io/api-reference/introduction/getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Write functions to interact with Label Studio API\n",
    "# - Create a project\n",
    "# - Import tasks\n",
    "# - Export annotations\n",
    "\n",
    "# Note: Requires running Label Studio locally\n",
    "# pip install label-studio\n",
    "# label-studio start\n",
    "\n",
    "import requests\n",
    "\n",
    "class LabelStudioClient:\n",
    "    def __init__(self, url='http://localhost:8080', api_key=None):\n",
    "        self.url = url\n",
    "        self.api_key = api_key\n",
    "        self.headers = {'Authorization': f'Token {api_key}'}\n",
    "    \n",
    "    def create_project(self, title, config):\n",
    "        \"\"\"Create a new annotation project.\"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def import_tasks(self, project_id, tasks):\n",
    "        \"\"\"Import tasks to a project.\"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def export_annotations(self, project_id, tasks, preannotated_from_fields=None):\n",
    "        \"\"\"Export annotations from a project.\"\"\"\n",
    "        # Your code here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Label Studio Setup**: Creating annotation interfaces with XML configs\n",
    "2. **Annotation Guidelines**: Writing clear, comprehensive guidelines with edge cases\n",
    "3. **Percent Agreement**: Simple but limited measure of annotator agreement\n",
    "4. **Cohen's Kappa**: Agreement metric that accounts for chance (2 annotators)\n",
    "5. **Fleiss' Kappa**: Extension for multiple annotators\n",
    "6. **IoU**: Measuring agreement for spatial annotations\n",
    "7. **Majority Voting**: Resolving disagreements between annotators\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Kappa >= 0.8** is the target for production annotation tasks\n",
    "- Clear guidelines with examples reduce disagreements\n",
    "- Multiple annotators + adjudication = higher quality labels\n",
    "- Different metrics for different annotation types\n",
    "\n",
    "### Next Week\n",
    "\n",
    "Week 4: Optimizing Labeling with Active Learning, Weak Supervision, and LLMs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
